{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import load, displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets \n",
    "\n",
    "with open('ner_spacy_fmt_datasets.pickle', 'rb') as f:\n",
    "    ner_spacy_fmt_datasets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.blank(\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(nlp.create_pipe('ner'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, annotations in ner_spacy_fmt_datasets:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0 {'ner': 45910.67175821347}\n",
      "Losses at iteration 1 {'ner': 44738.910790195376}\n",
      "Losses at iteration 2 {'ner': 44312.70587632521}\n",
      "Losses at iteration 3 {'ner': 44141.957184502346}\n",
      "Losses at iteration 4 {'ner': 43960.33697548103}\n",
      "Losses at iteration 5 {'ner': 44131.965996890154}\n",
      "Losses at iteration 6 {'ner': 43771.28000052126}\n",
      "Losses at iteration 7 {'ner': 43777.94996309585}\n",
      "Losses at iteration 8 {'ner': 43668.77543842289}\n",
      "Losses at iteration 9 {'ner': 43243.99606156396}\n",
      "Losses at iteration 10 {'ner': 43331.28969894315}\n",
      "Losses at iteration 11 {'ner': 43564.74727642169}\n",
      "Losses at iteration 12 {'ner': 43278.27375075633}\n",
      "Losses at iteration 13 {'ner': 43406.35800886545}\n",
      "Losses at iteration 14 {'ner': 43166.70321452029}\n",
      "Losses at iteration 15 {'ner': 42833.017318742066}\n",
      "Losses at iteration 16 {'ner': 43176.65926181083}\n",
      "Losses at iteration 17 {'ner': 42930.17800784978}\n",
      "Losses at iteration 18 {'ner': 43062.6399440933}\n",
      "Losses at iteration 19 {'ner': 42981.88144029334}\n",
      "Losses at iteration 20 {'ner': 42765.46697336065}\n",
      "Losses at iteration 21 {'ner': 42741.35320583259}\n",
      "Losses at iteration 22 {'ner': 42802.008482709934}\n",
      "Losses at iteration 23 {'ner': 42524.622840971075}\n",
      "Losses at iteration 24 {'ner': 42585.65080330237}\n",
      "Losses at iteration 25 {'ner': 42649.07686543428}\n",
      "Losses at iteration 26 {'ner': 42433.67836329287}\n",
      "Losses at iteration 27 {'ner': 42490.94476154539}\n",
      "Losses at iteration 28 {'ner': 42717.72603418251}\n",
      "Losses at iteration 29 {'ner': 42762.45193017861}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(ner_spacy_fmt_datasets)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(ner_spacy_fmt_datasets, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "    \n",
    "    print(\"Losses at iteration {}\".format(iteration), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SELUBUNG, Pendeta Yeremia Zanambani, Kabupaten Intan, Gabungan Pencari Fakta (TGPF, Intan Jaya)\n",
      "Entities [('SELUBUNG', 'ORGANIZATION'), ('Pendeta Yeremia Zanambani', 'PERSON'), ('Kabupaten Intan', 'LOCATION'), ('Gabungan Pencari Fakta (TGPF', 'ORGANIZATION'), ('Intan Jaya', 'LOCATION')]\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "doc = nlp(\"SELUBUNG yang menyelimuti kasus penembakan yang menewaskan Pendeta Yeremia Zanambani di Kabupaten Intan Jaya, Papua kian terkuak. Hasil investigasi Tim Gabungan Pencari Fakta (TGPF) kasus tersebut menyatakan bahwa penembakan di Intan Jaya diduga dilakukan oleh aparat keamanan.\")\n",
    "print(doc.ents)\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to nlp_id_checkpoint_2020_10_26\n"
     ]
    }
   ],
   "source": [
    "# save model \n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('nlp_id_checkpoint_2020_10_26')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from nlp_id_checkpoint_2020_10_26\n"
     ]
    }
   ],
   "source": [
    "# load existing model \n",
    "output_dir = 'nlp_id_checkpoint_2020_10_26'\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Bali', 'LOCATION')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_updated(\"Kementerian Perhubungan tidak mewajibkan rapid test COVID-19 untuk perjalanan darat lintas daerah, kecuali untuk tujuan Bali. Termasuk, dalam periode cuti bersama.\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Kementerian Perhubungan tidak mewajibkan rapid test COVID-19 untuk perjalanan darat lintas daerah, kecuali untuk tujuan \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Bali\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOCATION</span>\n",
       "</mark>\n",
       ". Termasuk, dalam periode cuti bersama. </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('PT Waskita Karya', 'ORGANIZATION'), ('KPK', 'ORGANIZATION'), ('Hugua', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_updated(\"Empat saksi terkait korupsi proyek infrastruktur fiktif yang dikerjakan PT Waskita Karya (Persero) Tbk absen dari panggilan KPK hari ini. Seorang di antaranya mantan Bupati Wakatobi, Hugua.\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Empat saksi terkait korupsi proyek infrastruktur fiktif yang dikerjakan \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PT Waskita Karya\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANIZATION</span>\n",
       "</mark>\n",
       " (Persero) Tbk absen dari panggilan \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    KPK\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORGANIZATION</span>\n",
       "</mark>\n",
       " hari ini. Seorang di antaranya mantan Bupati Wakatobi, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hugua\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
